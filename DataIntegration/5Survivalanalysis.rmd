#段雨畅2021213053008

# 5
___


# 统计匹配问题中的辅助变量选择
___


**马塞洛·德奥拉齐奥**

意大利国家统计研究所，意大利罗马

**马尔科·迪齐奥**

意大利国家统计研究所，意大利罗马

**毛罗·斯坎努**

意大利国家统计研究所，意大利罗马

## 内容
5.1 简介

5.2 匹配变量的选择

5.2.1 基于联想的传统方法

5.2.2 通过不确定性选择匹配变量减少

5.2.3 示例

5.2.4 受惩罚的不确定度测量

5.3 欧洲社会调查数据模拟

5.4 结论

参考文献

___

## **5.1简介**
统计匹配（SM，有时称为数据融合或综合匹配）旨在结合所参考的不同样本调查中的可用信息，当两个样本不相交时，分配给相同的目标群体。正式地设Y和Z是两个随机变量；统计匹配技术估计联合概率分布函数（Y，Z）的目标，或其中一个参数（例如列联表或回归系数）

当：

（i）Y和Z不是在调查中共同观察到的，Y是在含有n$_{\mbox{ A}}$个个体的样本A中观察到，Z是在含有n$_{\mbox{ B}}$个个体的样本B中观察到；

（ii）A和B是独立的，两个样本中的单位不重叠（不可能使用记录链接）；

（iii）A和B都观察到一组附加变量X。

这个问题最早是在[3]中以三变量(X,Y,Z)的高斯分布为例进行方法学研究的。三变量(X,Y,Z)在高斯分布的情况下，首次从方法上研究了这个问题。关于这个问题缺乏可识别性的研究可以追溯到[24]。缺少可识别性的影响是对联合(Y,Z)分布的多个同样合理的估计是可用的。传统的方法是隐含地或明确地引入假设，使模型可识别，以获得模型参数的唯一估计。通常的假设是在X的条件下，然而，这是一个很强的假设，即鉴于手头的数据，Y和Z的独立性。鉴于手头的数据，这是个很强的假设，不能被检验。为了避免引入假设，使推论更加可信，许多论文已经开始研究如何在考虑到非唯一性的情况下进行推论，考虑到统计匹配中估计的非唯一性，(也见[12])。这些推论是关于找出哪些感兴趣的真实参数与我们所做的观察相一致。这组数值被命名为"不确定性区域"[15]或 "部分识别区域"[31,17]。基于部分识别区域的推理，在其他背景下使用，例如计量经济学[30]和社会科学[20]。值得注意的是，"不确定区域"不应该与 "部分识别区域 "混淆。不应与"置信区间"确定的区域相混淆。在第一种情况下，它们产生于联合分布的部分可识别性；在第二种情况下，区间是由观测的抽样性质决定的,关于这两种不确定性的联合分析的结果在[19]和[28]中有所描述。

在统计匹配中，Rubin[25]定义了一种非正确的贝叶斯方法来探索不确定区域。这种方法在[23]中通过提出适当的贝叶斯方法而得到简化。Moriarty和Scheuren[21]探索了一套同样合理的解决方案，将所有可估计的参数固定为通过一致估计器获得的估计值；这种方法并不完全令人满意，因为有些结果是不可接受的（例如，高斯变量的协方差矩阵为负无限）。使用最大似然估计器来估计可识别的参数，然后找到相应的似然岭（即同样最大似然估计的集合），避免了在这个集合中包括不可接受的解决方案的可能性（见[16]）。使用不确定性一词来描述似然岭的宽度是在[16]中首次提出的。它在不同情况下的属性和它的估计以及对估计者的渐进性质的研究都在[11]。与最小推理概念相关的其他方法在[33]中概述。尽管不确定性是一个有助于描述我们在统计匹配背景下离可识别性有多远的概念，但本章的想法是，它也可以被用来用于操作性目的，也就是用于SM中的变量选择。更确切地说、假设两个调查A和B观察到足够多的共同变量X。大量的共同变量X的情况是很常见的。例如，在社会调查中，许多社会-经济变量 （居住地、年龄、性别、职业、教育和婚姻状况、户主的特征、家庭的特征等等）。等）都是可用的。是否应该将所有这些变量用于匹配目的？可能会出现问题；例如，众所周知，分类变量的数量越多，产生稀疏表格的风险就越大、 也就是每个单元格的观察值很少或为零的表格。更为普遍的是，大量的变量和相应的类别可能会对估计的效率产生影响。对估计的效率产生影响。在这种情况下，主要问题包括同时估计大量可识别的参数,经典的变量选择方法是基于对辅助变量X在(Y,Z)方面的解释能力的分析，例如，通过分析残差，对(Y,Z)的解释能力进行分析。例如，通过分析模型的残差。然而，经典方法需要对(Y,Z)进行联合观测，而在统计匹配的情况下，这些观测是不可用的。统计匹配的情况下无法获得。其结果是，它通常被作为一种次优的方法；也就是说，要分别选择解释Y和Z的变量X。事实上，现有的数据允许在这两个独立的 数据集。在本章中，我们声称可以通过不确定性的概念来找到匹配变量的适当选择，也就是选择那些使不确定性最小的变量那些使不确定性区域最小化的变量。为了避免选择所有可用的共同变量，应该考虑对不确定性的惩罚性措施 应该被考虑在内。在本章中，我们研究了基于表的稀疏性的惩罚。

___
## **5.2匹配变量的选择**

数据源A和B可能共享许多共同的变量X。SM时，不是所有的X变量都会被使用，而只是最重要的那些。选择最相关的X$_{\mbox{ M}}$（X$_{\mbox{ M}}$⊆X），称为匹配变量。通常通过咨询主题专家和适当的统计方法来进行（见[14]）。

匹配变量的选择应该在多变量的意义上进行。这就要求有一个数据源，其中(X,Y,Z)被观察到。不幸的是，在基本的SM框架中，这是不可能的，因为(X,Y,Z)从来没有被联合观测过，因此，选择是通过在两个数据集A和B中分别选择变量来进行的。我们的建议是，在选择匹配变量时进行独特的分析，方法是 我们的建议是，在选择匹配变量时进行独特的分析，寻找对减少Y和Z之间的不确定性最有效的共同变量集。匹配变量，避免选择过多的X变量。导致大量的参数需要估计。

### **5.2.1基于联想的传统方法**

变量选择方法一般是基于关联/依赖分析和因变量的残差分析。分析，以及对因变量残差的分析。如前所述，在基本的SM框架中，这是不允许的，因为缺乏 (Y,Z)的联合观测，A只允许研究Y和X之间的关系，而Z和X之间的关系则可以在B中研究。考虑到这些前提，对A和B分别进行了两个分析来选择相应的X变量，然后将这两个独立的分析结果结合起来。然后将两个独立的分析结果结合起来，一般来说，可以应用以下规则：

<center>X$_{\mbox{Y}}$ ∩ X$_{\mbox{Z}}$  ⊆ X$_{\mbox{M}}$ ⊆ X$_{\mbox{Y}}$∪ X$_{\mbox{Z}}$</center>


其中X$_{\mbox{Y}}$（X$_{\mbox{Y}}$⊆X）和X$_{\mbox{Z}}$（X$_{\mbox{Z}}$⊆X）是共同变量的子集 分别能更好地解释Y和Z。交叉点X$_{\mbox{Y}}$ ∩X$_{\mbox{Z}}$提供了如果与X$_{\mbox{Y}}$∪X$_{\mbox{Z}}$相比，提供了一个较小的匹配变量子集；这是实现解析的一个重要特征。这是实现简明性的重要特征。例如，在一个距离热甲板中，太多的匹配变量会在最终结果中引入不必要的额外噪音。不幸的是，X$_{\mbox{Y}}$ ∩X$_{\mbox{Z}}$的风险在于 的风险是，一个目标变量的最佳预测因子将被排除，如果它们不在另一个目标变量的预测器子集中，而且，在某些情况下，交集可能会被排除。此外，在某些情况下，交叉点可能是空的。由于这个原因，最终的匹配变量子集匹配变量的最终子集X$_{\mbox{M}}$通常是一个折衷的结果，而主题专家和数据分析师的贡献则是 为了确定"最佳"子集，主题专家和数据分析师的贡献是非常重要的"最好的子集"。

识别X$_{\mbox{Y}}$的最简单程序是计算Y和每个可用的预测因子X之间的成对相关/关联度。对B也应进行同样的分析，以确定Z的最佳预测因子。为了确定最终的非线性关系，考虑等级（Spearman等级相关系数）可能很方便。一个有趣的建议--可以在[18]中找到--包括查看与回归模型rank(Y)vs. rank(X)相关的调整后的${R^{2}}$（未经调整的${R^{2}}$ 对应于平方的Spearman等级相关系数）。当X是一个分类的名义变量时，它被认为是回归模型rank(Y )对dummies(X)的调整${R^{2}}$ 。

当反应和预测因素都是分类的，那么基于Chi-square的关联度量（Cramer's V）或方差比例减少的度量（见Goodman-Kruskalλ或τ）。可以考虑基于Chiquare的关联测量（Cramer'sV）或按比例减少方差的测量（例如，Goodman-Kruskal λ或τ）。坂本 和Akaike[26]建议使用Akaike信息准则（AIC），也就是说，最好的预测器是能够提供最准确的信息的。即最佳预测器是给出AIC统计量最小的那个。好的 基于AIC的选择程序的优点是，它还可以识别预测器的最佳组合。

有时，可以通过拟合模型来确定重要的预测因子,然后运行程序来选择最佳预测因子。选择子集X$_{\mbox{Y}}$也可以要求非参数程序如分类树和回归树[6]，或者更好的是随机森林[7]。它为预测因子提供了一个重要性的衡量标准。然而，随机森林的拟合在计算上要求很高，一些作者警告说，在选择最佳预测因子时不要使用预测因子的重要性衡量标准。(见，例如，[29]）。
