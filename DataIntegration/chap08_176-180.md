### 以下是王钦瑶的部分
#### 8.2.2 可识别性

我们所分析的这类模型具有可识别性的一个明显的必要条件是，独立参数的数量不大于观测到的不同剖面的数量 $\underline{y}$ - 1。因此，具有3个或更少明显二进制变量和结构零的潜在类模型本质上是不可识别的。事实上，在这种情况下，我们最多可以观察到7个不同的轮廓，而最简单的模型(8.3)将有7个参数。为了使用这样的模型，我们可以对参数施加一些限制，或者将一些协变量作为显式变量添加到模型中。

据我们所知，还没有发现这类模型可识别的一般充要条件。子类模型的结果可以在[32]和[2]中找到。然而，存在局部可识别的充要条件:Goodman[14]定义了一个模型，如果最大似然解在解周围的某个小区间(即 $\epsilon$ -邻域)内是唯一的，则该模型是局部可识别的。这比全局可识别性限制更少，在全局可识别性中，MLE必须在整个参数空间中惟一。Goodman证明了局部可识别性的充分必要条件是期望响应模式关于参数的雅可比矩阵是全列秩的。大多数用于潜在类别分析的计算机程序都包含对这种情况的检查。

#### 8.2.3 EM算法

为了计算我们模型的最大似然估计(MLE)，我们假设存在一个完整列联表 $T^{* }= \left[ n_{x,y}\right]$ ，我们观察了其中的边际计数 $T$ 。我们定义一个带有参数 { $\lambda$ } 的 $T^ {* }$ 的对数线性模型，然后，一旦我们初始化了参数{ $\lambda$ }的估计(或者， $T^ {* }$ 的单元格)，我们就可以设置一个EM算法，迭代如下两步:

E-step:计算 $T^ {* }$ 条件下在观测边缘 $T$ 上的细胞的期望计数和{ $\lambda$ }的当前估计，对于每个轮廓 $y$ ，计算估计的后验概率 $\widehat{\pi}_ {x|\underline{y}}$ 和

 $$ \widehat{n}_ {x\underline{y}}=n_{\underline{y}}\widehat{\pi}_ {x|\underline{y}}$$

M-step：更新对数线性模型参数{ $\lambda$ }在E-step中计算的频率 $\widehat{n}_ {x\underline{y}}$ 上的MLE。

在M-step中，可以使用迭代比例拟合(IPF)算法(algorithm)来获得{ $\lambda$ }的最大似得。实际上，已经证明IPF不需要完全收敛:在每个M-step上进行一次迭代就足以使整个EM算法收敛(参见，例[30])。

理论上可以采用两种方法进行估计过程:条件估计和无条件估计(见[11])。在条件方法中，我们将在结构零单元上有条件地最大化对数似然，这将从算法的两个步骤中排除。一旦EM算法收敛，{ $\lambda$ }的当前的MLE被用来估计 $T^ {* }$ 的每个单元，包括结构零单元 $n_{x,\underline{0}}$ 。另一方面，无条件方法相当于在每次迭代中估计和更新结构零单元 $n_{\underline{0}}$ 。为此，我们应该使用一个“嵌套”EM算法(见[6])，它有两个嵌套周期，其中外层周期初始化并更新结构零单元格 $\widehat{n}_ {\underline{0}}$ 的估计，而内部周期则根据当前 $T \cup \widehat{n}_ {\underline{0}}$ 更新完整数据 $T^ {* }$ 。(包括单元格 $\widehat{n}_ {x,\underline{0}}$ )的估计。然而，由于这两种方法是等效的，第一种方法更可取，因为它在计算上更容易。

当模型可分解时，我们可以避免对数线性表达式，并且算法的M-step可以以封闭形式求解，因为我们有一个参数最大化完全对数似然的解析公式。也就是说，由于完全对数似然可以表示为

$$ \pi _ {x,y}= \pi _ {C_{1}}\prod _ {i=2}^{g}\pi _ {C_{i}|S_{i}}$$

我们可以用 $\widehat { n }_ {C _ { i }} / \widehat { n }_ {S _ { i }}$ 估计每个参数 $\pi _ {C_{i}|S_{i}}$  ，其中 $\widehat { n }_ {C _ { i }}$ 和 $\widehat { n }_ {S _ { i }}$ 是通过将步骤E得到的 $\widehat{n}_ {x\underline{y}}$ 除以两个子图中不包括的变量来得到的。

例如，在估计模型(8.6)中，M-step可以计算出:

 $\widehat{\pi}_ {x}= \frac{\widehat{n}_ {x}}{n_{obs}}$  ，  $\widehat{\pi}_ {a|x}= \frac{\widehat{n}_ {xa}}{\widehat{n}_ {x}}$ ， $\widehat{\pi}_ {b|x}= \frac{\widehat{n}_ {xb}}{\widehat{n}_ {x}}$ ， $\widehat{\pi}_ {cd|x}= \frac{\widehat{n}_ {xcd}}{\widehat{n}_ {x}}$ 。

其中

$\widehat{n}_ {x}= \sum _ {\underline{y} \neq \underline{0}}\widehat{n}_ {x\underline{y}}$ ， $\widehat{n}_ {xa}= \sum _ {b,c,d}\widehat{n}_ {xabcd}$ ， $\widehat{n}_ {xb}= \sum _ {a,c,d}\widehat{n}_ {xabcd}$ ， $\widehat{n}_ {xcd}= \sum _ {a,b}\widehat{n}_ {xabcd}$ 。

#### 8.2.4 固定的参数

我们采用的潜在类模型的无监督方法暗示了对每个来源的质量(即错误率的评估)的中立观点。由于这个原因，当没有可用的审计样本时，潜在类别模型有时被用来估计一组源中的错误率。但是，如果我们有一些关于信息源错误率的可靠信息，我们可以通过固定一些参数将其包含在模型中。例如，如果我们知道源 $Y$ 没有错误捕获，我们可以将相应的参数 $\pi _ {1|0}^{Y|X}$ 固定为零。

对参数施加一些限制的另一个原因是，通过假设一个更节俭的模型来节省大量的自由。在某些情况下，为了有一个可识别的模型，这是必要的。也就是说，也就是说，当可用列表的数量不允许使用更复杂的模型时。

只要我们考虑可分解模型，并且我们想施加的限制类型是将参数固定为特定值或在参数之间施加相等约束，我们就可以通过一个相当简单的过程最大化受约束的似然。Goodman在1974年提出了一个在大多数情况下都有效的简单方法。Mooijaart和van der Heijden[24]随后改进了Goodman的算法，并提出通过在对数似然函数中添加拉格朗日乘子来估计概率具有相等和/或固定值限制的(可分解)模型。

#### 8.2.5 不同成分的混合物

我们想指出我们正在考虑的对数线性模型的一个假设，它限制了我们可以表示的关系:即两个分量分布(以 $X=1$ 和 $X = 0$ 为条件)具有相同的依赖结构。在某些情况下，这一点是不合适的。

例如，在模型 $[AX]$ $[BX]$ 下，无论 $X$ 等于 $1$ 还是 $0$ , $A$ 和 $B$ 都是有条件独立的。如果我们添加项 $[AB]$ ，我们将允许 $A$ 和 $B$ 有条件地依赖于 $X$ ，但无论 $X = 1$ 还是 $X = 0$ ，相互作用的参数 $[AB]$ 都是相同的。如果我们也添加项 $[ABX]$ ，那么 $A$ 和 $B$ 之间的相互作用在 $X = 1$ 和 $X = 0$ 上不一定是有条件地相同的，但这种包括高阶相互作用的策略可能会导致过度参数化。不难想象这样一种情况， $A$ 和 $B$ 在一个子种群中是独立的，所以，比如说，

$\pi _ {ab|0}= \pi _ {a|0}\pi _ {b|0}$ ，但 $\pi _ {ab|1}\neq \pi _ {a|1}\pi _ {b|1}$ 。

例如，假设我们有两个具有相同问题的来源，识别二进制状态。假设“ $0$ ”状态与应答者无关，而“ $1$ ”表示可能被隐藏的敏感状态。在这种情况下，条件独立假设在 $X = 0$ 时成立，但当 $X = 1$ 时，被调查者在两次调查中的说谎倾向是一致的。

在这种情况下，我们想在模型 $[ABX]$ 中固定约束 $\pi _ {ab|0}= \pi _ {a|0}\pi _ {b|0}$ ，但这不是估计算法中容易处理的约束(见8.2.4节)。一种可能的解决方法是放弃对数线性形式，并定义两个不相等分量分布的混合模型:

$$ \pi _ {ab}= \pi _ {0}\pi _ {a|0}\pi _ {b|0}+ \pi _ {1}\pi _ {ab|1}$$

通过这种方式，两个组件可以独立定义(最终为两个不同的对数线性模型)。这种泛化对估计过程没有任何困难。事实上，在算法的步骤M中，我们可以分别处理与步骤E中估计的值 $\widehat{n}_ {1,\underline{y}}$ 有关的完全对数似然值的部分和与值 $\widehat{n}_ {0,\underline{y}}$ 有关的部分。

这类模型的使用例子可以在记录链接的混合模型中找到，其中[18]提出了一个模型，其中相对于真匹配的对数线性组件仅具有主效应，而第二个组件具有所有主效应和双向交互。也就是说，该模型在 $X = 1$ 时满足条件独立假设。其思想是，如果我们有一个真正的匹配，在各种显式变量上的分歧是随机错误的结果，而在不匹配的情况下，分歧更有可能发生在逻辑集中([42])。

#### 8.2.6 模型选择

模型选择是捕获-再捕获中的一个关键点，因为通常对总体规模的估计对参数选择的微小变化非常敏感。解决这个问题的典型方法是使用信息标准来比较模型。特别是，可以使用逐步的(向后或向前)过程来进行基于AIC或BIC的参数选择。然而，我们在专门为潜在类模型设计的文献中有几个结果。这方面的大部分贡献都集中在使用局部独立模型(8.2)时检测清单变量之间的成对依赖关系。第一种方法是比较任意一对源 $Y^{\prime}$ 和 $Y^{\prime \prime}$ 的估计和观测二元边缘 $\pi ^{Y^{\prime}Y^{\prime \prime}}$ 和 $\widehat{\pi} ^{Y^{\prime}Y^{\prime \prime}}$ 。

Qu等人[28]建议检验估计的相关性

$$\widehat{Corr}_ {Y ^ { \prime } Y ^ { \prime \prime }} = \frac { \widehat{\pi} _ {11} ^{Y^{\prime}Y^{\prime\prime}}-\widehat{\pi}_ {1}^{Y^{\prime}}\widehat{\pi}_ {1}^{Y^{\prime\prime}} } { \sqrt { \widehat{\pi}_ {1}^{Y^{\prime}} ( 1 - \widehat{\pi}_ {1}^{Y^{\prime}}) \widehat{\pi}_ {1}^{Y^{\prime\prime}} ( 1 - \widehat{\pi}_ {1}^{Y^{\prime\prime}} )  }}$$

和观察到的，并计算每对变量的差值。最终使用自举置信区间可以发现显著差异，相应的成对交互项可以添加到对数线性模型中。

二元残差(BVR)计算两个二元分布之间的卡方距离:

$$\sum _ { i , j \in  { [0 , 1] } ^ { 2 } } \frac { ( n _ {i,j}^{Y^{\prime}Y^{\prime\prime}}- \widehat{n} _ {i,j}^{Y^{\prime}Y^{\prime\prime}})^2 } {  \widehat{n} _ {i,j}^{Y^{\prime}Y^{\prime\prime}} }$$

显然(见[25])BVR指数不具有[40]中所述的渐近 $\chi^2$ 分布。然后，可以再次使用自举置信区间。

另一个近年来受到关注的统计数据是预期参数变化(EPC)，被称为结构方程建模中的修改指数，它允许人们检测模型中不包含的参数更相关([25]，[26])。我们考虑一个由参数 $\theta_0$ 定义的空模型，和一个由参数 $θ = (θ_0，θ_1)$ 定义的替代模型。空模型被视为参数 $θ_1$ 被限制为等于零的模型。如果它们被释放，然后EPC估计参数 $θ_1$ 的位移。设 $\underline{s}θ_1$ 为固定参数的积分向量，$\underline{s}θ_1 =∂l(θ)/∂θ_1$ ，设 $\widehat{I}(θ)$ 为期望的信息矩阵。然后，如果空模型成立，则测试统计量

$$\underline{s}_ {\theta _ {1}}^{T}\widehat{I}(\theta)^{-1}\underline{s}_ {\theta _ {1}}$$，

在 $θ_1 = 0$ 处的值渐近地分布为 $χ^2$ ，其自由度等于 $θ_1$ 中参数的数量。

话虽如此，上述任何工具选择的最佳模型并不总是给出最佳 $N_1$ 估计的模型，而且可能是具有相似拟合优度水平的不同模型导致对总体规模的估计非常不同。

然后，在模型评估中纳入一些关于主题的考虑是很重要的。参数的估计应该基于我们对数据的先验知识进行评估，某些参数值不合理的模型应该被丢弃。

注意，第8.2.3节算法得到的参数是指结构零单元上有条件截断的分布。因此，为了对参数有一个清晰的解释，我们首先必须估计完全列联表 $T^{* }$ ，并推导出完全分布的参数。

以下内容为万亚珂完成：
# 8.3 观察到的捕获概率的异质性

## 8.3.1 协变量的使用

即使我们对列表之间的所有现有依赖关系进行建模，也可能存在仍会有一些无法解释的变化，这是因为个人被捕获的倾向性。一些捕获概率的异质性可以由一组协变量来解释，这些协变量必须包括在模型中。

如果未能对个体捕获概率中的列表依赖性或异质性进行建模，通常会导致对种群大小的有偏估计。然而请注意，这两个方面是不容易分开的。一个具有足够数量参数y的对数线性模型可以代表 $$ \left\{ 0,1 \right\} ^{k}$$ 上的任何分布；因此，原则上总是有可能找到具有任何拟合优度水平的（可能是过度参数化的）模型。如果在模型中不包含个体异质性的相关来源，就会导致列表之间的虚假依赖关系，并且可能会错误地包含在模型中的虚假参数。

单个协变量的使用通常包括在模型中的两种方式之一（关于对数线性模型中两种方法的分析，见[37])。第一种方法是使用观察到的协变量来影响模型参数。也就是说，我们重新参数化潜伏类成员的概率（混合权重$$\pi _ { x }$$)和/或条件捕获概率$$ \pi _{y|x}$$，以一组协变量为基础。在这种方法中，协变量明确地具有解释变量的作用；它们既可以是分类的，也可以是连续的，而且有可能限制它们对特定参数的影响。通过将对数线性模型的公式转化为多项logit模型，可以通过几种方式实现参数化模型（见[39]、[43]、[35]、[36]）。

第二种方法，只有在所有协变量都是分类的时才可用，将它们作为清单变量。也就是说，我们为联合分布$$ \pi _{y,v}$$定义了一个模型，其中V是我们的协变量集。通过这种方式，我们可以对所有清单变量（捕获或协变量）之间的关系进行建模，并通过几种方式调整它们的依赖图。

必须指出的是在这种方法中，协变量并不一定存在影响$$N{1}$$的估计，除非它与其他证明的依赖关系变量是经过精心选择的。估计的不变性与对数线性模型的坍陷性有关。假设我们在一组变量Y上有一个对数线性模型M，我们考虑变量Y‘的一个子集，以及相对诱导对数线性模型M’，它只保持M相对于Y’中的所有变量的参数。如果模型M‘对每个单元格$$n{y^{\prime}}$$的估计等于模型M所对应的边际估计，则称M在Y’以上是可折叠的。设G为M的依赖图，H为Y/Y ’上的诱导子图，即未包含在Y‘中的G的变量图。Asmussen和Edwards [3]找到了可折叠的充分必要条件，即如果H的每个连通分量的G边界在M在Y‘上是可折叠的，构成M’的生成器。那么，如果$$ \pi _{x,y,v}$$的对数线性模型可在（X，Y）上折叠，那么协变量不会改变$$N{1}$$的估计。关于这方面，也可以看本书的第七章。

举个例子，假设我们想在图8.1的模型中包含协变量V。然后,建立模型

$$ \left[ AX \right] \left[ BX \right] \left[ CDX \right] \left[ CDV\right]$$

在{A、B、C、D、X}上是可折叠的，因为C和D在图中是连通的，而V不会改变$$N{1}$$的最终估计。相反，模型

$$ \left[ AX \right] \left[ BX \right] \left[ CDX \right] \left[ AV \right] \left[ CV\right]$$

是不可折叠的，因为A和C不连接，而V会影响$$N{1}$$的估计。

然而，请注意，这只对观察到的变量有效：如果V与潜在的X相互作用，那么它通常不会是可折叠的。例如，模型

$$ \left[ AVX \right] \left[ BX \right] \left[ CDX \right] ,$$

其中列表A的过覆盖误差随V变化，即使{A,X}是图的一个派系，在{A、B、C、D、X}上不可折叠。

值得注意的是，如果我们对估计由协变量确定的每个地层的种群大小感兴趣，那么即使是一个可折叠的模型也可能是有用的，因为我们可以简单地估计所有值v的数量$$n \frac{Y_{}}{0}\frac{V}{v}$$。

## 8.3.2 不完整的列表

捕获概率的异质性的一个极端情况是由不可捕获的亚种群来表示的，即被捕获的概率为零的单位。在这里，我们处理已确定的亚种群的情况，即已知的一个或多个列表（但不是全部）无法接受的单位。这个问题在实践中经常发生，因为当一个源针对我们感兴趣的人群的一个子集时，它就会发生。我们不想丢失那些我们将称之为“不完整来源”中可用的信息；然而，我们需要考虑它们在模型中的不完整性。也就是说，如果我们忽略列表的结构不完全性，并将不可捕获的单位视为抽样零，那么对种群规模的结果估计可能会有严重的偏差。

我们假设我们对每个不完整的列表都无法捕获的亚种群（或地层）有一个完美的知识。因此，U被划分为不同的不完整列表不运行的层。这可以通过分层变量S形式化划分U。由于S是已知的，我们可以采用一种简单的方法，用不同的模型分别估计每个地层的亚种群的大小。每个模型将只包括相对地层上的列表，然后将估计数相加将得出总人口规模估计数。然而，这种方法将极大地限制我们可以考虑的可识别模型的范围，因为我们经常遇到以下情况：不完整的列表有单独的目标，只有一个小子集，所有列表操作。例如，考虑4个列表和2个层的简单情况，一个是所有列表操作，另一个是只有3个列表操作。在第二层中，我们可以只考虑几个简单的对数线性模型，而一个潜在的类模型是不可识别的。

另一种方法是将未被不完整列表所覆盖的单元的不可观察捕获视为缺失信息。然后，将这些地层中的单元视为部分分类，即好像捕获历史部分缺失，然后我们估计缺失的部分。当然，潜在的假设是，在其他列表的条件下，每个不完整列表在地层中的分布是相同的，也就是说，我们假设一个随机缺失（MAR）机制(见[19]，章.3.2见[39])。如果这个假设是站得住脚的，我们可以在一个模型中包含所有记录，即使存在不完整的列表。另一方面，当假设不正确时，所得到的估计可能会有偏差。

使用对数线性模型来推断列联表中的部分缺失信息是众所周知的（见[30]）。这种方法在捕获中的应用，在[44]中也进行了探索。扩展到存在潜在变量的情况并没有任何困难，因为它不需要对第8.2.3节中描述的算法进行很少的修改。但是，请注意，这两个方面（潜在变量X和不完整的列表）不能很容易地用对数线性模型单独来解决。事实上，由于X与所有明显的变量相互作用，因此（X、Y、S）的对数线性模型将不能折叠到（Y、S）上。因此，如果有人想初步处理由于不完整的列表而造成的缺失，然后在完整的数据上估计潜在的类模型，他们应该定义和估计两个不同的对数线性模型。

假设分层变量S在一个有限的集合S = {s1，s2，...}中取值，以识别不同的不完整列表集合不工作的不同地层。对于每个地层s，我们观察到一组不同的边际计数$$T_{s}$$。我们假设存在一个完整的列联表$$T^{}= \left[ n{x,y,s}\right] ,$$，并且我们想要估计

$$N _ { 1 } = \sum _ { y \in\{ 0 , 1 \} ^ { k }{s\in S} } n _ { 1,y, s }$$

然后，在EM算法的E步中，对于每个层s，我们根据观察到的计数$$T _ { s } .$$有条件地计算$$T ^ { * }$$相应细胞的期望计数$$ \widehat{n}_{x,y,s}$$。

应注意$$T ^ { * }$$模型的依赖结构的选择。事实上，在许多情况下，S，或一些模式，可以构成一个有用的协变量来解释一些异质性捕获概率，我们想包括一些交互参数与其他变量，以这样一种方式模型不能折叠（X，Y）。然而，当且仅当处于某一地层的概率独立于所有不在该地层中运行的列表时，不完整列表的MAR假设才成立。因此，一个S与不完整的列表变量相互作用的模型将违反MAR假设，如果没有进一步的假设，它将无法被识别。相反，一个S不与任何其他变量相互作用的模型意味着缺失机制是完全随机的（MCAR）。

在存在不完整列表的情况下，我们有多个结构零单元格需要估计。它们的数量在每一层中的变化取决于不完整的列表的数量。形式上，设S=表示Y1，...，Ym，m < k不运行的地层。然后，对于该层，我们有以下$$2^{m+1}$$结构零细胞：

$$\{ n\begin{array}  { l  }  { X , Y _ { 1 } , \cdots , Y _ { m },  \cdots,Y _ { m } + 1 ,  Y _ { k  } , S } \\ { x,y _ { 1 }, \cdots,y_{m},0, \cdots,0,s } \end{array} _ { ( x , y_{1},\cdots,y_{m} \in \left\{ 0,1 \right\} ^{m+1}) }$$

在EM算法中排除，并在其收敛后进行估计。

例如，考虑列表A、B、C和D，其中列表A不完整，而S∈{s1，s2}表示A是否可操作。观察到的边际计数为$$T _ { 1 } = [ n _ { a b c d s _ { 1 } } ]$$和$$T _ { 1 } = [ n _ { b c d s_ { 2 } } ]$$。$$T ^ { * }$$的结构性零细胞为：

$$n{x0000s{1}}^{XABC}$$ , $$n_{x0000s_{2}}^{XABCDS}$$ ,and $$n_{x1000s_{2}}^{XABCDS}$$ , $$forx \in \left\{ 0,1\right\}$$

观测到的不完整数据的对数似然值为：

$$ \sum _{a,b,c,d}n_{abcds_{1}}\log \pi _{abcds_{1}}+ \sum _{b,c,d}n_{bcds_{2}}\log \pi _{bcds_{2}}$$

而EM算法的指定方式如下：

1. 随机初始化条件概率{$$ \widehat{\pi}_{x|abcds_{1}}$$}和{$$ \widehat{\pi}_{x|abcds_{2}}$$}；

2. 通过计算估计完整的列联表$$ \widehat{T}^{*}= \left[ n_{xabcds}\right]$$，排除结构零单元格（8.7）

$$\widehat{n}_{xabcds_{1}}=n_{abcds_{1}}\widehat{\pi}_{x|abcds_{1}}$$  $$ \forall cells s.t.(a,b,c,d)\neq(0,0,0,0)$$

$$\widehat{n}_{xabcds_{2}}=n_{abcds_{1}}\widehat{\pi}_{x|abcds_{2}}$$  $$ \forall cells s.t.(a,b,c,d)\neq(0,0,0,0)$$

3. 根据不可观测的结构零单元（8.7），计算当前$$ \widehat{T}^{*}$$上所选择的对数线性模型的MLE；

4. 更新条件概率的当前估计值

$$ \widehat{\pi}_{x|s_{1}abcd}$$=$$\frac{\widehat{n}_{xa_{n}abds_{1}}}{ \sum _{a,b,c,d}\widehat{n}_{xabcds_{1}}},$$，

$$ \widehat{\pi}_{x|s_{2}abcd}$$=$$\frac{\widehat{n}_{xa_{n}abds_{2}}}{ \sum _{a,b,c,d}\widehat{n}_{xabcds_{2}}},$$，



5.重复2-4次，直到收敛。

在收敛后，我们估计了单元格（8.7）。

请注意，在我们正在考虑的一些模型中，可以对不完整的列表（或一般缺失的数据）采用一种更简单的方法。事实上，如果模型是可分解的，并且缺失的部分和观察到的部分形成了不同的似然因素，我们可以采用所谓的“全信息最大似然”方法。在这种情况下，一个单位的值只是意味着可能性中没有相对参数。例如，如果B是模型中的一个不完整的列表（或在任何情况下都是一个有缺失值的列表）
$$\pi _ { A b c d s } = \sum _ {x \in \ left \{ 0,1\ right \}} \pi _ { x } \pi _ { a | x } \pi _ { b | x } \pi _ { c d | x } \pi _ { s }$$

然后，所有有B缺失值的单位，根据数量对可能性的贡献

$$ \sum _{x}\pi _{x}\pi _{a|x}\pi _{cd|x},$$

不需要进一步的计算工作。在局部独立模型（8.2）下，任何缺失值的模式都可以用这种方式来处理

不幸的是，并不是所有缺失的模式都可以按照这段中的描述来处理。事实上，我们需要一个所有列表同时运行的子种群，以便能够估计一个潜在的类模型。原因依赖于这样一个事实，即即使在模型（8.3）的最简单的情况下，最小的充分统计量是观察到的计数 $$n{y}$$ ，我们不能通过充分性实现数据缩减（见[12]）。因此，如果在我们的数据中，我们不能观察到 ${n_y}$ ，而只是一些边际计数，我们的估计可能是有偏差的。在没有观察到高阶联合分布的情况下（即，在对数线性术语中，不能估计高阶的相互作用），这些项的影响很低，并且估计 $N{1}$ 的最终偏差可以忽略不计。

# 8.4评估对潜在类别的解释

我们的模型背后的基本思想本质上是隐藏在概率记录链接问题背后的相同思想，其中二进制潜在变量旨在识别真匹配和假匹配。也就是说，在我们的模型中，我们隐式地假设有两个具有不同分布的子种群代表所期望的情况和虚假的情况，并且两个分布的混合将代表我们的数据。

当然，这种对潜在变量的解释是可以进行验证的。事实上，有限混合模型在捕获-再捕获中的常用用法是对个体间未观察到的捕获概率的异质性进行建模，这在包含可用的协变量后仍无法解释。其基本原理是通过建模观察不能直接识别的不同亚种群数据来增强拟合。通常，组件的数量的选择是一个在拟合优度和参数数量的简约性之间的平衡的问题。因此，两个潜在组件的混合不一定代表我们期望的感兴趣的子种群（范围内和范围外单元）。例如，如果我们有两组明显不同的单位，它们具有非常不同的捕获概率和相似的错误捕获率，那么一个两类模型很可能会识别这两组，而不是错误捕获和真实捕获。

当然，我们对潜在变量的解释的合理性可以通过后验来检验。事实上，除非模型被严重错误指定，否则估计的概率$$ \widehat{\pi}_{y|x}$$和估计的后验概率$$ \widehat{\pi}_{x|y}$$将有助于我们解释潜在变量，识别潜在类别的类别性质，并理解我们所做的假设是否成立。

我们对这个问题的先验知识应该指导我们评估这些可能性的合理性。例如，在许多感兴趣的情况下，我们期望几乎所有源的假捕获率都很低，以及它们之间的积极相互作用。如果明显变量都与X高度相关，并且潜在类被很好地分离出来，也就是说，如果类$$ \widehat{\pi}{y|x}$$的概率接近于0或1，那么这就构成了支持我们的论点的证据。特别是，我们期望 $$ \widehat{\pi}{1|1}$$ 和 $$ \widehat{\pi}{0|0}$$ 的概率接近于1。相反，如果所有源在两个潜在类中捕获概率都超过0.5，而$$ \widehat{\pi}{1|1}$$和$$ \widehat{\pi}_{0|0}$$远非一个，这将将潜在类解释为具有不同被捕获倾向的两个亚种群。

请注意，所有有限的混合模型最多都是可识别的，直到潜在类的标签的任意排列（标签切换问题）。因此，总是需要通过分析估计的参数来识别代表真实捕获的潜在类，即使模型的可拉性不是一个问题。

