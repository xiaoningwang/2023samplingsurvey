### 以下是王钦瑶的部分
#### 8.2.2 可识别性

我们所分析的这类模型具有可识别性的一个明显的必要条件是，独立参数的数量不大于观测到的不同剖面的数量 $\underline{y}$ - 1。因此，具有3个或更少明显二进制变量和结构零的潜在类模型本质上是不可识别的。事实上，在这种情况下，我们最多可以观察到7个不同的轮廓，而最简单的模型(8.3)将有7个参数。为了使用这样的模型，我们可以对参数施加一些限制，或者将一些协变量作为显式变量添加到模型中。

据我们所知，还没有发现这类模型可识别的一般充要条件。子类模型的结果可以在[32]和[2]中找到。然而，存在局部可识别的充要条件:Goodman[14]定义了一个模型，如果最大似然解在解周围的某个小区间(即 $\epsilon$ -邻域)内是唯一的，则该模型是局部可识别的。这比全局可识别性限制更少，在全局可识别性中，MLE必须在整个参数空间中惟一。Goodman证明了局部可识别性的充分必要条件是期望响应模式关于参数的雅可比矩阵是全列秩的。大多数用于潜在类别分析的计算机程序都包含对这种情况的检查。

#### 8.2.3 EM算法

为了计算我们模型的最大似然估计(MLE)，我们假设存在一个完整列联表 $T^{* }= \left[ n_{x,y}\right]$ ，我们观察了其中的边际计数 $T$ 。我们定义一个带有参数 { $\lambda$ } 的 $T^ {* }$ 的对数线性模型，然后，一旦我们初始化了参数{ $\lambda$ }的估计(或者， $T^ {* }$ 的单元格)，我们就可以设置一个EM算法，迭代如下两步:

E-step:计算 $T^ {* }$ 条件下在观测边缘 $T$ 上的细胞的期望计数和{ $\lambda$ }的当前估计，对于每个轮廓 $y$ ，计算估计的后验概率 $\widehat{\pi}_ {x|\underline{y}}$ 和

 $$ \widehat{n}_ {x\underline{y}}=n_{\underline{y}}\widehat{\pi}_ {x|\underline{y}}$$

M-step：更新对数线性模型参数{ $\lambda$ }在E-step中计算的频率 $\widehat{n}_ {x\underline{y}}$ 上的MLE。

在M-step中，可以使用迭代比例拟合(IPF)算法(algorithm)来获得{ $\lambda$ }的最大似得。实际上，已经证明IPF不需要完全收敛:在每个M-step上进行一次迭代就足以使整个EM算法收敛(参见，例[30])。

理论上可以采用两种方法进行估计过程:条件估计和无条件估计(见[11])。在条件方法中，我们将在结构零单元上有条件地最大化对数似然，这将从算法的两个步骤中排除。一旦EM算法收敛，{ $\lambda$ }的当前的MLE被用来估计 $T^ {* }$ 的每个单元，包括结构零单元 $n_{x,\underline{0}}$ 。另一方面，无条件方法相当于在每次迭代中估计和更新结构零单元 $n_{\underline{0}}$ 。为此，我们应该使用一个“嵌套”EM算法(见[6])，它有两个嵌套周期，其中外层周期初始化并更新结构零单元格 $\widehat{n}_ {\underline{0}}$ 的估计，而内部周期则根据当前 $T \cup \widehat{n}_ {\underline{0}}$ 更新完整数据 $T^ {* }$ 。(包括单元格 $\widehat{n}_ {x,\underline{0}}$ )的估计。然而，由于这两种方法是等效的，第一种方法更可取，因为它在计算上更容易。

当模型可分解时，我们可以避免对数线性表达式，并且算法的M-step可以以封闭形式求解，因为我们有一个参数最大化完全对数似然的解析公式。也就是说，由于完全对数似然可以表示为

$$ \pi _ {x,y}= \pi _ {C_{1}}\prod _ {i=2}^{g}\pi _ {C_{i}|S_{i}}$$

我们可以用 $\widehat { n }_ {C _ { i }} / \widehat { n }_ {S _ { i }}$ 估计每个参数 $\pi _ {C_{i}|S_{i}}$  ，其中 $\widehat { n }_ {C _ { i }}$ 和 $\widehat { n }_ {S _ { i }}$ 是通过将步骤E得到的 $\widehat{n}_ {x\underline{y}}$ 除以两个子图中不包括的变量来得到的。

例如，在估计模型(8.6)中，M-step可以计算出:

 $\widehat{\pi}_ {x}= \frac{\widehat{n}_ {x}}{n_{obs}}$  ，  $\widehat{\pi}_ {a|x}= \frac{\widehat{n}_ {xa}}{\widehat{n}_ {x}}$ ， $\widehat{\pi}_ {b|x}= \frac{\widehat{n}_ {xb}}{\widehat{n}_ {x}}$ ， $\widehat{\pi}_ {cd|x}= \frac{\widehat{n}_ {xcd}}{\widehat{n}_ {x}}$ 。

其中

$\widehat{n}_ {x}= \sum _ {\underline{y} \neq \underline{0}}\widehat{n}_ {x\underline{y}}$ ， $\widehat{n}_ {xa}= \sum _ {b,c,d}\widehat{n}_ {xabcd}$ ， $\widehat{n}_ {xb}= \sum _ {a,c,d}\widehat{n}_ {xabcd}$ ， $\widehat{n}_ {xcd}= \sum _ {a,b}\widehat{n}_ {xabcd}$ 。

#### 8.2.4 固定的参数

我们采用的潜在类模型的无监督方法暗示了对每个来源的质量(即错误率的评估)的中立观点。由于这个原因，当没有可用的审计样本时，潜在类别模型有时被用来估计一组源中的错误率。但是，如果我们有一些关于信息源错误率的可靠信息，我们可以通过固定一些参数将其包含在模型中。例如，如果我们知道源 $Y$ 没有错误捕获，我们可以将相应的参数 $\pi _ {1|0}^{Y|X}$ 固定为零。

对参数施加一些限制的另一个原因是，通过假设一个更节俭的模型来节省大量的自由。在某些情况下，为了有一个可识别的模型，这是必要的。也就是说，也就是说，当可用列表的数量不允许使用更复杂的模型时。

只要我们考虑可分解模型，并且我们想施加的限制类型是将参数固定为特定值或在参数之间施加相等约束，我们就可以通过一个相当简单的过程最大化受约束的似然。Goodman在1974年提出了一个在大多数情况下都有效的简单方法。Mooijaart和van der Heijden[24]随后改进了Goodman的算法，并提出通过在对数似然函数中添加拉格朗日乘子来估计概率具有相等和/或固定值限制的(可分解)模型。

#### 8.2.5 不同成分的混合物

我们想指出我们正在考虑的对数线性模型的一个假设，它限制了我们可以表示的关系:即两个分量分布(以 $X=1$ 和 $X = 0$ 为条件)具有相同的依赖结构。在某些情况下，这一点是不合适的。

例如，在模型 $[AX]$ $[BX]$ 下，无论 $X$ 等于 $1$ 还是 $0$ , $A$ 和 $B$ 都是有条件独立的。如果我们添加项 $[AB]$ ，我们将允许 $A$ 和 $B$ 有条件地依赖于 $X$ ，但无论 $X = 1$ 还是 $X = 0$ ，相互作用的参数 $[AB]$ 都是相同的。如果我们也添加项 $[ABX]$ ，那么 $A$ 和 $B$ 之间的相互作用在 $X = 1$ 和 $X = 0$ 上不一定是有条件地相同的，但这种包括高阶相互作用的策略可能会导致过度参数化。不难想象这样一种情况， $A$ 和 $B$ 在一个子种群中是独立的，所以，比如说，

$\pi _ {ab|0}= \pi _ {a|0}\pi _ {b|0}$ ，但 $\pi _ {ab|1}\neq \pi _ {a|1}\pi _ {b|1}$ 。

例如，假设我们有两个具有相同问题的来源，识别二进制状态。假设“ $0$ ”状态与应答者无关，而“ $1$ ”表示可能被隐藏的敏感状态。在这种情况下，条件独立假设在 $X = 0$ 时成立，但当 $X = 1$ 时，被调查者在两次调查中的说谎倾向是一致的。

在这种情况下，我们想在模型 $[ABX]$ 中固定约束 $\pi _ {ab|0}= \pi _ {a|0}\pi _ {b|0}$ ，但这不是估计算法中容易处理的约束(见8.2.4节)。一种可能的解决方法是放弃对数线性形式，并定义两个不相等分量分布的混合模型:

$$ \pi _ {ab}= \pi _ {0}\pi _ {a|0}\pi _ {b|0}+ \pi _ {1}\pi _ {ab|1}$$

通过这种方式，两个组件可以独立定义(最终为两个不同的对数线性模型)。这种泛化对估计过程没有任何困难。事实上，在算法的步骤M中，我们可以分别处理与步骤E中估计的值 $\widehat{n}_ {1,\underline{y}}$ 有关的完全对数似然值的部分和与值 $\widehat{n}_ {0,\underline{y}}$ 有关的部分。

这类模型的使用例子可以在记录链接的混合模型中找到，其中[18]提出了一个模型，其中相对于真匹配的对数线性组件仅具有主效应，而第二个组件具有所有主效应和双向交互。也就是说，该模型在 $X = 1$ 时满足条件独立假设。其思想是，如果我们有一个真正的匹配，在各种显式变量上的分歧是随机错误的结果，而在不匹配的情况下，分歧更有可能发生在逻辑集中([42])。

#### 8.2.6 模型选择

模型选择是捕获-再捕获中的一个关键点，因为通常对总体规模的估计对参数选择的微小变化非常敏感。解决这个问题的典型方法是使用信息标准来比较模型。特别是，可以使用逐步的(向后或向前)过程来进行基于AIC或BIC的参数选择。然而，我们在专门为潜在类模型设计的文献中有几个结果。这方面的大部分贡献都集中在使用局部独立模型(8.2)时检测清单变量之间的成对依赖关系。第一种方法是比较任意一对源 $Y^{\prime}$ 和 $Y^{\prime \prime}$ 的估计和观测二元边缘 $\pi ^{Y^{\prime}Y^{\prime \prime}}$ 和 $\widehat{\pi} ^{Y^{\prime}Y^{\prime \prime}}$ 。

Qu等人[28]建议检验估计的相关性

$$\widehat{Corr}_ {Y ^ { \prime } Y ^ { \prime \prime }} = \frac { \widehat{\pi} _ {11} ^{Y^{\prime}Y^{\prime\prime}}-\widehat{\pi}_ {1}^{Y^{\prime}}\widehat{\pi}_ {1}^{Y^{\prime\prime}} } { \sqrt { \widehat{\pi}_ {1}^{Y^{\prime}} ( 1 - \widehat{\pi}_ {1}^{Y^{\prime}}) \widehat{\pi}_ {1}^{Y^{\prime\prime}} ( 1 - \widehat{\pi}_ {1}^{Y^{\prime\prime}} )  }}$$

和观察到的，并计算每对变量的差值。最终使用自举置信区间可以发现显著差异，相应的成对交互项可以添加到对数线性模型中。

二元残差(BVR)计算两个二元分布之间的卡方距离:

$$\sum _ { i , j \in  { [0 , 1] } ^ { 2 } } \frac { ( n _ {i,j}^{Y^{\prime}Y^{\prime\prime}}- \widehat{n} _ {i,j}^{Y^{\prime}Y^{\prime\prime}})^2 } {  \widehat{n} _ {i,j}^{Y^{\prime}Y^{\prime\prime}} }$$

显然(见[25])BVR指数不具有[40]中所述的渐近 $\chi^2$ 分布。然后，可以再次使用自举置信区间。

另一个近年来受到关注的统计数据是预期参数变化(EPC)，被称为结构方程建模中的修改指数，它允许人们检测模型中不包含的参数更相关([25]，[26])。我们考虑一个由参数 $\theta_0$ 定义的空模型，和一个由参数 $θ = (θ_0，θ_1)$ 定义的替代模型。空模型被视为参数 $θ_1$ 被限制为等于零的模型。如果它们被释放，然后EPC估计参数 $θ_1$ 的位移。设 $\underline{s}θ_1$ 为固定参数的积分向量，$\underline{s}θ_1 =∂l(θ)/∂θ_1$ ，设 $\widehat{I}(θ)$ 为期望的信息矩阵。然后，如果空模型成立，则测试统计量

$$\underline{s}_ {\theta _ {1}}^{T}\widehat{I}(\theta)^{-1}\underline{s}_ {\theta _ {1}}$$，

在 $θ_1 = 0$ 处的值渐近地分布为 $χ^2$ ，其自由度等于 $θ_1$ 中参数的数量。

话虽如此，上述任何工具选择的最佳模型并不总是给出最佳 $N_1$ 估计的模型，而且可能是具有相似拟合优度水平的不同模型导致对总体规模的估计非常不同。

然后，在模型评估中纳入一些关于主题的考虑是很重要的。参数的估计应该基于我们对数据的先验知识进行评估，某些参数值不合理的模型应该被丢弃。

注意，第8.2.3节算法得到的参数是指结构零单元上有条件截断的分布。因此，为了对参数有一个清晰的解释，我们首先必须估计完全列联表 $T^{* }$ ，并推导出完全分布的参数。
